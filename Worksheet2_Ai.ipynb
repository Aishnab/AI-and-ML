{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cDlM-SzpitDC",
        "outputId": "24f273b2-1cb7-422d-83f0-1801484d0a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax function passed the test case!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def softmax(z):\n",
        " z_max = np.max(z, axis=1, keepdims=True)\n",
        " exp_z = np.exp(z - z_max)\n",
        " return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "z_test = np.array([[2.0, 1.0, 0.1], [1.0, 1.0, 1.0]])\n",
        "softmax_output = softmax(z_test)\n",
        "# Verify if the sum of probabilities for each row is 1 using assert\n",
        "row_sums = np.sum(softmax_output, axis=1)\n",
        "# Assert that the sum of each row is 1\n",
        "assert np.allclose(row_sums, 1), f\"Test failed: Row sums are {row_sums}\"\n",
        "print(\"Softmax function passed the test case!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Compute the softmax probabilities for a given input matrix.\n",
        "Parameters:\n",
        "z (numpy.ndarray): Logits (raw scores) of shape (m, n), where\n",
        "\n",
        "- m is the number of samples.\n",
        "- n is the number of classes.\n",
        "\n",
        "Returns:\n",
        "numpy.ndarray: Softmax probability matrix of shape (m, n), where\n",
        "each row sums to 1 and represents the probability\n",
        "distribution over classes.\n",
        "\n",
        "Notes:\n",
        "- The input to softmax is typically computed as: z = XW + b.\n",
        "- Uses numerical stabilization by subtracting the max value per row.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "uy5-MSqKrhM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_softmax(X, W, b):\n",
        "  z = np.dot(X, W) + b\n",
        "  probabilities = softmax(z)\n",
        "  predicted_classes = np.argmax(probabilities, axis=1)\n",
        "  return predicted_classes"
      ],
      "metadata": {
        "id": "FJwQrxOxlslL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "Predict the class labels for a set of samples using the trained softmax model.\n",
        "Parameters:\n",
        "X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the\n",
        "number of features.\n",
        "W (numpy.ndarray): Weight matrix of shape (d, c), where c is the number of classes.\n",
        "b (numpy.ndarray): Bias vector of shape (c,).\n",
        "Returns:\n",
        "numpy.ndarray: Predicted class labels of shape (n,), where each value is the index of the\n",
        "predicted class.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xSNq0AAfs3gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The test function ensures that the predicted class labels have the same number of elements as the input samples, verifying that the model produces a valid output shape.\n",
        "# Define test case\n",
        "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]]) # Feature matrix (3 samples, 2 features)\n",
        "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]]) # Weights (2 features, 3 classes)\n",
        "b_test = np.array([0.1, 0.2, 0.3]) # Bias (3 classes)\n",
        "# Expected Output:\n",
        "# The function should return an array with class labels (0, 1, or 2)\n",
        "y_pred_test = predict_softmax(X_test, W_test, b_test)\n",
        "# Validate output shape\n",
        "assert y_pred_test.shape == (3,), f\"Test failed: Expected shape (3,), got {y_pred_test.shape}\"\n",
        "# Print the predicted labels\n",
        "print(\"Predicted class labels:\", y_pred_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aLXG4pRluVg",
        "outputId": "6c027f1d-5cb3-4934-ff46-c867e72f9bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class labels: [1 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_softmax(y_pred, y):\n",
        "  \"\"\"\n",
        "  Compute the cross-entropy loss for a single sample.\n",
        "  Parameters:\n",
        "  y_pred (numpy.ndarray): Predicted probabilities of shape (c,) for a single sample,\n",
        "\n",
        "  where c is the number of classes.\n",
        "\n",
        "  y (numpy.ndarray): True labels (one-hot encoded) of shape (c,), where c is the number of classes.\n",
        "  Returns:\n",
        "  float: Cross-entropy loss for the given sample.\n",
        "  \"\"\"\n",
        "  epsilon=1e-12\n",
        "  y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "  loss = -np.sum(y * np.log(y_pred))\n",
        "  return loss"
      ],
      "metadata": {
        "id": "d1zMZV1qu0fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "  # This test case Compares loss for correct vs. incorrect predictions.\n",
        "  # Expects low loss for correct predictions.\n",
        "  # Expects high loss for incorrect predictions.\n",
        "  # Define correct predictions (low loss scenario)\n",
        "y_true_correct = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) # True one-hot labels\n",
        "y_pred_correct = np.array([[0.9, 0.05, 0.05],[0.1, 0.85, 0.05],[0.05, 0.1, 0.85]]) # High confidence in the correct class\n",
        "\n",
        "  # Define incorrect predictions (high loss scenario)\n",
        "y_pred_incorrect = np.array([[0.05, 0.05, 0.9], [0.1, 0.05, 0.85],[0.85, 0.1, 0.05]])# Highly confident in the wrong class\n",
        "\n",
        "  # Compute loss for both cases\n",
        "loss_correct = loss_softmax(y_pred_correct, y_true_correct)\n",
        "loss_incorrect = loss_softmax(y_pred_incorrect, y_true_correct)\n",
        "  # Validate that incorrect predictions lead to a higher loss\n",
        "assert loss_correct < loss_incorrect, f\"Test failed: Expected loss_correct < loss_incorrect, but got{loss_correct:.4f} >= {loss_incorrect:.4f}\"\n",
        "  # Print results\n",
        "print(f\"Cross-Entropy Loss (Correct Predictions): {loss_correct:.4f}\")\n",
        "print(f\"Cross-Entropy Loss (Incorrect Predictions): {loss_incorrect:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGEWfXGS1u64",
        "outputId": "ca759636-3905-4092-d784-3246277a1627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Entropy Loss (Correct Predictions): 0.4304\n",
            "Cross-Entropy Loss (Incorrect Predictions): 8.9872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "implement cost function\n"
      ],
      "metadata": {
        "id": "UfqXeMQ02eim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_softmax(X, y, W, b):\n",
        "  \"\"\"\n",
        "  Compute the average softmax regression cost (cross-entropy loss) over all samples.\n",
        "  Parameters:\n",
        "  X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the\n",
        "  number of features.\n",
        "  y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c), where n is the number of\n",
        "  samples and c is the number of classes.\n",
        "  W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "  b (numpy.ndarray): Bias vector of shape (c,).\n",
        "  Returns:\n",
        "  float: Average softmax cost (cross-entropy loss) over all samples.\n",
        "  \"\"\"\n",
        "  n = X.shape[0]  # Get the number of samples\n",
        "  z = np.dot(X, W) + b\n",
        "  y_pred = softmax(z)\n",
        "  total_loss = -np.sum(y * np.log(y_pred))\n",
        "# Return average loss\n",
        "  return total_loss / n"
      ],
      "metadata": {
        "id": "MWFRd8vQ3Ovb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The test case assures that the cost for the incorrect prediction should be higher than for thecorrect prediction, confirming that the cost function behaves as expected.\n",
        "import numpy as np\n",
        "# Example 1: Correct Prediction (Closer predictions)\n",
        "X_correct = np.array([[1.0, 0.0], [0.0, 1.0]]) # Feature matrix for correct predictions\n",
        "y_correct = np.array([[1, 0], [0, 1]]) # True labels (one-hot encoded, matching predictions)\n",
        "W_correct = np.array([[5.0, -2.0], [-3.0, 5.0]]) # Weights for correct prediction\n",
        "b_correct = np.array([0.1, 0.1]) # Bias for correct prediction\n",
        "# Example 2: Incorrect Prediction (Far off predictions)\n",
        "X_incorrect = np.array([[0.1, 0.9], [0.8, 0.2]]) # Feature matrix for incorrect predictions\n",
        "y_incorrect = np.array([[1, 0], [0, 1]]) # True labels (one-hot encoded, incorrect predictions)\n",
        "W_incorrect = np.array([[0.1, 2.0], [1.5, 0.3]]) # Weights for incorrect prediction\n",
        "b_incorrect = np.array([0.5, 0.6]) # Bias for incorrect prediction\n",
        "# Compute cost for correct predictions\n",
        "cost_correct = cost_softmax(X_correct, y_correct, W_correct, b_correct)\n",
        "# Compute cost for incorrect predictions\n",
        "cost_incorrect = cost_softmax(X_incorrect, y_incorrect, W_incorrect, b_incorrect)\n",
        "# Check if the cost for incorrect predictions is greater than for correct predictionsassert cost_incorrect > cost_correct, f\"Test failed: Incorrect cost {cost_incorrect} is not greaterthan correct cost {cost_correct}\"\n",
        "# Print the costs for verification\n",
        "print(\"Cost for correct prediction:\", cost_correct)\n",
        "print(\"Cost for incorrect prediction:\", cost_incorrect)\n",
        "print(\"Test passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbP-8qXD4erI",
        "outputId": "e5c67e0d-fc7b-4087-a74b-f6108d588419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost for correct prediction: 0.0006234364133349324\n",
            "Cost for incorrect prediction: 0.29930861359446115\n",
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gradient\n"
      ],
      "metadata": {
        "id": "tXkCP6mL3Z8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_softmax(X, y, W, b):\n",
        "  \"\"\"\n",
        "  Compute the gradients of the cost function with respect to weights and biases.\n",
        "  Parameters:\n",
        "  X (numpy.ndarray): Feature matrix of shape (n, d).\n",
        "  y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n",
        "  W (numpy.ndarray): Weight matrix of shape (d, c).\n",
        "  b (numpy.ndarray): Bias vector of shape (c,).\n",
        "  Returns:\n",
        "  tuple: Gradients with respect to weights (d, c) and biases (c,).\n",
        "  \"\"\"\n",
        "  n,d = X.shape\n",
        "  z = np.dot(X,W)+b\n",
        "  y_pred = softmax(z)\n",
        "  grad_w = np.dot(X.T,y_pred-y)/n\n",
        "  grad_b = np.sum(y_pred-y,axis=0)/n\n",
        "  return grad_w,grad_b"
      ],
      "metadata": {
        "id": "hs8G5AB02a-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Define a simple feature matrix and true labels\n",
        "X_test = np.array([[0.2, 0.8], [0.5, 0.5], [0.9, 0.1]]) # Feature matrix (3 samples, 2 features)\n",
        "y_test = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) # True labels (one-hot encoded, 3 classes)\n",
        "# Define weight matrix and bias vector\n",
        "W_test = np.array([[0.4, 0.2, 0.1], [0.3, 0.7, 0.5]]) # Weights (2 features, 3 classes)\n",
        "b_test = np.array([0.1, 0.2, 0.3]) # Bias (3 classes)\n",
        "# Compute the gradients using the function\n",
        "grad_W, grad_b = compute_gradient_softmax(X_test, y_test, W_test, b_test)\n",
        "# Manually compute the predicted probabilities (using softmax function)\n",
        "z_test = np.dot(X_test, W_test) + b_test\n",
        "y_pred_test = softmax(z_test)\n",
        "# Compute the manually computed gradients\n",
        "grad_W_manual = np.dot(X_test.T, (y_pred_test - y_test)) / X_test.shape[0]\n",
        "grad_b_manual = np.sum(y_pred_test - y_test, axis=0) / X_test.shape[0]\n",
        "# Assert that the gradients computed by the function match the manually computed gradients\n",
        "assert np.allclose(grad_W, grad_W_manual), f\"Test failed: Gradients w.r.t. W are not equal.\\nExpected: {grad_W_manual}\\nGot: {grad_W}\"\n",
        "assert np.allclose(grad_b, grad_b_manual), f\"Test failed: Gradients w.r.t. b are not equal.\\nExpected: {grad_b_manual}\\nGot: {grad_b}\"\n",
        "# Print the gradients for verification\n",
        "print(\"Gradient w.r.t. W:\", grad_W)\n",
        "print(\"Gradient w.r.t. b:\", grad_b)\n",
        "print(\"Test passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ6uDfVz43ZR",
        "outputId": "3932d92d-b27c-40d2-ef2f-c009e3838032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient w.r.t. W: [[ 0.1031051   0.01805685 -0.12116196]\n",
            " [-0.13600547  0.00679023  0.12921524]]\n",
            "Gradient w.r.t. b: [-0.03290036  0.02484708  0.00805328]\n",
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "johZgTJ2tqAJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}